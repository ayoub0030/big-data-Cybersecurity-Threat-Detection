{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e29bea-a244-4f51-9b69-09ad57751638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch 13 written to InfluxDB with 167 entries\n",
      "✅ Batch 14 written to InfluxDB with 152 entries\n",
      "✅ Batch 15 written to InfluxDB with 167 entries\n",
      "✅ Batch 16 written to InfluxDB with 297 entries\n",
      "✅ Batch 17 written to InfluxDB with 296 entries\n",
      "✅ Batch 18 written to InfluxDB with 296 entries\n",
      "✅ Batch 19 written to InfluxDB with 296 entries\n",
      "✅ Batch 20 written to InfluxDB with 296 entries\n",
      "✅ Batch 21 written to InfluxDB with 297 entries\n",
      "✅ Batch 22 written to InfluxDB with 296 entries\n",
      "✅ Batch 23 written to InfluxDB with 297 entries\n",
      "✅ Batch 24 written to InfluxDB with 297 entries\n",
      "✅ Batch 25 written to InfluxDB with 296 entries\n",
      "✅ Batch 26 written to InfluxDB with 297 entries\n",
      "✅ Batch 27 written to InfluxDB with 296 entries\n",
      "✅ Batch 28 written to InfluxDB with 296 entries\n",
      "✅ Batch 29 written to InfluxDB with 297 entries\n",
      "✅ Batch 30 written to InfluxDB with 296 entries\n",
      "✅ Batch 31 written to InfluxDB with 68 entries\n",
      "✅ Batch 32 written to InfluxDB with 100 entries\n",
      "✅ Batch 33 written to InfluxDB with 91 entries\n",
      "✅ Batch 34 written to InfluxDB with 9 entries\n",
      "✅ Batch 35 written to InfluxDB with 100 entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from influxdb_client import InfluxDBClient as InfluxDBClientV2, Point, WriteOptions\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "# ========== Spark & Kafka Setup ==========\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomalyDetectionStream\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ========== Schema for Incoming Kafka Logs ==========\n",
    "log_schema = StructType() \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"source_ip\", StringType()) \\\n",
    "    .add(\"destination_ip\", StringType()) \\\n",
    "    .add(\"protocol\", StringType()) \\\n",
    "    .add(\"port\", StringType()) \\\n",
    "    .add(\"threat\", StringType()) \\\n",
    "    .add(\"user_agent\", StringType()) \\\n",
    "    .add(\"location\", StringType()) \\\n",
    "    .add(\"bytes_sent\", StringType()) \\\n",
    "    .add(\"bytes_received\", StringType())\n",
    "\n",
    "# ========== Load Model and Preprocessing Tools ==========\n",
    "model = joblib.load(\"best_model.joblib\")\n",
    "scaler = joblib.load(\"scaler.joblib\")\n",
    "encoders = joblib.load(\"encoders.joblib\")\n",
    "target_encoder = joblib.load(\"target_encoder.joblib\")\n",
    "\n",
    "# Get the original feature order from the scaler\n",
    "feature_order = scaler.feature_names_in_\n",
    "\n",
    "# ========== Read Stream from Kafka ==========\n",
    "df_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"logs\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "df_value = df_raw.selectExpr(\"CAST(value AS STRING)\")\n",
    "df_parsed = df_value.select(from_json(col(\"value\"), log_schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# ========== Batch Prediction and Write to InfluxDB ==========\n",
    "def predict_and_store(batch_df, batch_id):\n",
    "    pdf = batch_df.toPandas()\n",
    "    if pdf.empty:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Ensure we only keep columns we need and in correct order\n",
    "        pdf = pdf[list(feature_order) + [\"timestamp\"]]\n",
    "        pdf.dropna(subset=feature_order, inplace=True)\n",
    "        \n",
    "        if pdf.empty:\n",
    "            return\n",
    "\n",
    "        # Encode categorical columns\n",
    "        # for col_name in [\"source_ip\", \"destination_ip\", \"protocol\", \"user_agent\", \"location\", \"port\"]:\n",
    "        #     le = encoders[col_name]\n",
    "        #     pdf[col_name] = pdf[col_name].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "\n",
    "        # Encode categorical columns - FIXED VERSION\n",
    "        for col_name in [\"source_ip\", \"destination_ip\", \"protocol\", \"user_agent\", \"location\", \"port\"]:\n",
    "            le = encoders[col_name]\n",
    "            # Create a set of known classes for faster lookup\n",
    "            known_classes = set(le.classes_)\n",
    "            # Use vectorized operations instead of map+lambda\n",
    "            pdf[col_name] = pdf[col_name].apply(\n",
    "                lambda x: le.transform([x])[0] if x in known_classes else -1\n",
    "            )\n",
    "\n",
    "        # Convert numeric fields\n",
    "        pdf[\"bytes_sent\"] = pdf[\"bytes_sent\"].astype(float)\n",
    "        pdf[\"bytes_received\"] = pdf[\"bytes_received\"].astype(float)\n",
    "\n",
    "        # Prepare features in correct order\n",
    "        X = pdf[feature_order]\n",
    "        X_scaled = scaler.transform(X)\n",
    "        preds = model.predict(X_scaled)\n",
    "        pdf[\"predicted_threat\"] = target_encoder.inverse_transform(preds)\n",
    "\n",
    "        # ========== InfluxDB v2 Setup ==========\n",
    "        token = \"uCje_bdIW9K2MVKT8NVGOiqQhF0BpcZWR_IXheO1O91M9EgbjPJHpEQ0Ur5vlpnTACHO--b-AYzAVGbkqcjmSg==\"\n",
    "        org = \"myorg\"\n",
    "        bucket = \"mybucket\"\n",
    "        influx_host = \"http://influxdb:8086\"\n",
    "\n",
    "        influx_client = InfluxDBClientV2(\n",
    "            url=influx_host,\n",
    "            token=token,\n",
    "            org=org\n",
    "        )\n",
    "        write_api = influx_client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "        # Write each row as a point\n",
    "        for _, row in pdf.iterrows():\n",
    "            point = (\n",
    "                Point(\"network_logs\")\n",
    "                .tag(\"source_ip\", str(row[\"source_ip\"]))\n",
    "                .tag(\"destination_ip\", str(row[\"destination_ip\"]))\n",
    "                .tag(\"protocol\", str(row[\"protocol\"]))\n",
    "                .field(\"bytes_sent\", float(row[\"bytes_sent\"]))\n",
    "                .field(\"bytes_received\", float(row[\"bytes_received\"]))\n",
    "                .field(\"predicted_threat\", str(row[\"predicted_threat\"]))\n",
    "                .time(row[\"timestamp\"])\n",
    "            )\n",
    "            write_api.write(bucket=bucket, org=org, record=point)\n",
    "\n",
    "        print(f\"✅ Batch {batch_id} written to InfluxDB with {len(pdf)} entries\")\n",
    "        influx_client.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in batch {batch_id}: {e}\")\n",
    "\n",
    "# ========== Start Streaming ==========\n",
    "query = df_parsed.writeStream \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .foreachBatch(predict_and_store) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/anomaly_detection\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
