{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e29bea-a244-4f51-9b69-09ad57751638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from influxdb_client import InfluxDBClient as InfluxDBClientV2, Point, WriteOptions\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "print('start')\n",
    "# ========== Spark & Kafka Setup ==========\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomalyDetectionStream\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ========== Schema for Incoming Kafka Logs ==========\n",
    "log_schema = StructType() \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"source_ip\", StringType()) \\\n",
    "    .add(\"destination_ip\", StringType()) \\\n",
    "    .add(\"protocol\", StringType()) \\\n",
    "    .add(\"port\", StringType()) \\\n",
    "    .add(\"threat\", StringType()) \\\n",
    "    .add(\"user_agent\", StringType()) \\\n",
    "    .add(\"location\", StringType()) \\\n",
    "    .add(\"bytes_sent\", StringType()) \\\n",
    "    .add(\"bytes_received\", StringType())\n",
    "\n",
    "# ========== Load Model and Preprocessing Tools ==========\n",
    "model = joblib.load(\"best_model.joblib\")\n",
    "scaler = joblib.load(\"scaler.joblib\")\n",
    "encoders = joblib.load(\"encoders.joblib\")\n",
    "target_encoder = joblib.load(\"target_encoder.joblib\")\n",
    "\n",
    "# Get the original feature order from the scaler\n",
    "feature_order = scaler.feature_names_in_\n",
    "\n",
    "# ========== Read Stream from Kafka ==========\n",
    "df_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"logs\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "df_value = df_raw.selectExpr(\"CAST(value AS STRING)\")\n",
    "df_parsed = df_value.select(from_json(col(\"value\"), log_schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# ========== Batch Prediction and Write to InfluxDB ==========\n",
    "def predict_and_store(batch_df, batch_id):\n",
    "    pdf = batch_df.toPandas()\n",
    "    if pdf.empty:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Ensure we only keep columns we need and in correct order\n",
    "        pdf = pdf[list(feature_order) + [\"timestamp\"]]\n",
    "        pdf.dropna(subset=feature_order, inplace=True)\n",
    "        \n",
    "        if pdf.empty:\n",
    "            return\n",
    "\n",
    "        # Encode categorical columns\n",
    "        # for col_name in [\"source_ip\", \"destination_ip\", \"protocol\", \"user_agent\", \"location\", \"port\"]:\n",
    "        #     le = encoders[col_name]\n",
    "        #     pdf[col_name] = pdf[col_name].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "\n",
    "        # Encode categorical columns - FIXED VERSION\n",
    "        for col_name in [\"source_ip\", \"destination_ip\", \"protocol\", \"user_agent\", \"location\", \"port\"]:\n",
    "            le = encoders[col_name]\n",
    "            # Create a set of known classes for faster lookup\n",
    "            known_classes = set(le.classes_)\n",
    "            # Use vectorized operations instead of map+lambda\n",
    "            pdf[col_name] = pdf[col_name].apply(\n",
    "                lambda x: le.transform([x])[0] if x in known_classes else -1\n",
    "            )\n",
    "\n",
    "        # Convert numeric fields\n",
    "        pdf[\"bytes_sent\"] = pdf[\"bytes_sent\"].astype(float)\n",
    "        pdf[\"bytes_received\"] = pdf[\"bytes_received\"].astype(float)\n",
    "\n",
    "        # Prepare features in correct order\n",
    "        X = pdf[feature_order]\n",
    "        X_scaled = scaler.transform(X)\n",
    "        preds = model.predict(X_scaled)\n",
    "        pdf[\"predicted_threat\"] = target_encoder.inverse_transform(preds)\n",
    "\n",
    "        # ========== InfluxDB v2 Setup ==========\n",
    "        token = \"yfV-ntkcS4nMJYXIVf0HDsqqIlx0bnRQjDl2ECGRZTuBMndKX0s674uglHTRj9eDkDRrn5fv2lRYzbszPxQxuQ==\"\n",
    "        org = \"myorg\"\n",
    "        bucket = \"mybucket\"\n",
    "        influx_host = \"http://influxdb:8086\"\n",
    "\n",
    "        influx_client = InfluxDBClientV2(\n",
    "            url=influx_host,\n",
    "            token=token,\n",
    "            org=org\n",
    "        )\n",
    "        write_api = influx_client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "        # Write each row as a point\n",
    "        for _, row in pdf.iterrows():\n",
    "            point = (\n",
    "                Point(\"network_logs\")\n",
    "                .tag(\"source_ip\", str(row[\"source_ip\"]))\n",
    "                .tag(\"destination_ip\", str(row[\"destination_ip\"]))\n",
    "                .tag(\"protocol\", str(row[\"protocol\"]))\n",
    "                .field(\"bytes_sent\", float(row[\"bytes_sent\"]))\n",
    "                .field(\"bytes_received\", float(row[\"bytes_received\"]))\n",
    "                .field(\"predicted_threat\", str(row[\"predicted_threat\"]))\n",
    "                .time(row[\"timestamp\"])\n",
    "            )\n",
    "            write_api.write(bucket=bucket, org=org, record=point)\n",
    "\n",
    "        print(f\"✅ Batch {batch_id} written to InfluxDB with {len(pdf)} entries\")\n",
    "        influx_client.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in batch {batch_id}: {e}\")\n",
    "\n",
    "# ========== Start Streaming ==========\n",
    "query = df_parsed.writeStream \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .foreachBatch(predict_and_store) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/anomaly_detection\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4d6ade-a487-4487-8680-9f6d13af132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting influxdb-client\n",
      "  Downloading influxdb_client-1.49.0-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m901.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting reactivex>=4.0.4 (from influxdb-client)\n",
      "  Downloading reactivex-4.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (68.2.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.5.3->influxdb-client) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /opt/conda/lib/python3.11/site-packages (from reactivex>=4.0.4->influxdb-client) (4.8.0)\n",
      "Downloading influxdb_client-1.49.0-py3-none-any.whl (746 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.3/746.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading reactivex-4.1.0-py3-none-any.whl (218 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.6/218.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: reactivex, influxdb-client\n",
      "Successfully installed influxdb-client-1.49.0 reactivex-4.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## %pip install influxdb-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8e0e2a-2a93-45f1-b22c-4e53db584756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /opt/conda/lib/python3.11/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b0837-d0d9-4e9c-b975-92e3bba31e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
